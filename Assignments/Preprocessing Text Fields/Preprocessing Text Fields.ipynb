{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Text Fields\n",
    "\n",
    "This assignment walks through the basics of cleaning text fields with regex and pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries and Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the necessary libraries and dataset. The dataset we are using for this assignment is [Kaggle's spam SMS dataset](https://www.kaggle.com/uciml/sms-spam-collection-dataset). The only modifications that we've done to the dataset are to encode the CSV using UTF-8 so that it can be read by pandas, change ham and spam labels to 0 and 1 respectively, as well as clean up the dataset column labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sms = pd.read_csv(\"spam.csv\")\n",
    "sms.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis = 1, inplace = True)\n",
    "sms.rename(columns = {\"v1\": \"label\", \"v2\": \"text\"}, inplace = True)\n",
    "sms[\"label\"] = sms[\"label\"].map({\"ham\": 0, \"spam\": 1})\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first just look at the counts of the how many spam and ham labels we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms.groupby(by = [\"label\"]).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have a class imbalance, a naive classifier that always outputs ham is able to achieve an accuracy of 87%. To discourage this, we use the area under the ROC curve (AUROC) as our metric instead of just accuracy. A perfect classifier will have an AUROC of 1, while a naive classifier has an AUROC of 0.5. We can confirm this by calculating the ROC of our naive always ham classifier below. \n",
    "\n",
    "You do not need to understand how this metric works, as it is not the goal of this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(sms[\"label\"], [0] * len(sms.index))\n",
    "metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that the naive classifier indeed has an AUROC of 0.5. Let's see if we can do better than that by processing the text!\n",
    "\n",
    "Next, let's take a look at some of these text entries to see if we can find any differences between spam and ham texts by just reading them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First 15 Ham Texts:\\n\")\n",
    "for i, ham_text in enumerate(sms.loc[sms[\"label\"] == 0][\"text\"][:15]):\n",
    "    print(\"{}. {}\\n\".format(i + 1, ham_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First 15 Spam Texts:\\n\")\n",
    "for i, spam_text in enumerate(sms.loc[sms[\"label\"] == 1][\"text\"][:15]):\n",
    "    print(\"{}. {}\\n\".format(i + 1, spam_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this initial inspection of our dataset, we can see that our dataset isn't perfectly labeled. The sixth text in the ham texts is clearly a spam, despite being labeled as a ham text. Despite that, we can still see at least some differences in the texts. Spam texts generally contain more numbers (either phone numbers or codes), links, and miscellaneous capitalization/punctuation compared to ham texts.\n",
    "\n",
    "Let's see if we can use these indicators as features for a classifier!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Phone Numbers\n",
    "\n",
    "Our classifier's first feature will be the number of digits of the longest number in the text. For example, a phone number like 09061209465 will have a length of 11. Add this feature as a column called \"longestNum\" to the `sms` dataframe. \n",
    "\n",
    "*Hint: To apply regex to a pandas series, look into series.str.function.*<br>\n",
    "*Hint: Try to first find all numbers for a single text, and then search for the maximum length one.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN STUDENT SOLUTION\n",
    "\n",
    "# END STUDENT SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the distribution of this new feature for spam and ham texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(\n",
    "    data = sms,\n",
    "    x = \"longestNum\",\n",
    "    hue = \"label\"\n",
    ")\n",
    "plt.title(\"Distribution of characters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this plot, we can observe that this feature is pretty helpful in separating our data, as spam features tend to have longer numbers than ham texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Links\n",
    "\n",
    "In addition to just having numbers with more digits, spam texts also generally have more links than ham texts. However, one problem with trying to identify links is that they don't all take the same format. Links could have \"http\", \"www\", etc. as the prefix and \".com\", \".net\", etc. as the suffix. To simplify our search, assume that links will always have the following structure: \n",
    "\n",
    "\"www\" / \"http\" | text (at least 1 character) | \".com\" / \".net\"\n",
    "Prefix         | domain                      | suffix\n",
    "\n",
    "Note that the only character the text cannot contain is a space. Additionally, while the last part is referred to as a suffix, we do not require there to be a space after the suffix. Add a {0, 1} indicator for if text contains a link as a column called \"containsLink\" to the `sms` dataframe. \n",
    "\n",
    "*Hint: The pipe (|) operator serves as an or.*<br>\n",
    "*Hint: Parentheses may be helpful.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# BEGIN STUDENT SOLUTION\n",
    "\n",
    "# END STUDENT SOLUTION "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at these counts: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms.drop(\"longestNum\", axis = 1).groupby(by=[\"label\", \"containsLink\"]).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in total, we only found 52 links in our entire dataset; while this is far lower than we might have hoped, it's also not entirely unexpected, as we're looking at spam texts instead of spam emails. Additionally, despite the low counts, links are much more prevalent for spam texts as opposed to ham texts. We will keep this feature in, but will be far less helpful than our first feature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Magic Words\n",
    "\n",
    "One very naive way to process text is to have some features be indicators for if \"magic words\" you believe to be good separators exist in a piece of text. For example, \"free\" might be a good magic word, since many spam texts from our initial selection use that word. We define the set of the following magic words: {\"txt\", \"claim\", \"confirm\", \"free\", \"reply\", \"xxx\", \"reward\", \"award\"}. Let our magic words be case insensitive, so \"Free\" and \"free\" both count towards the indicator.\n",
    "\n",
    "However, we want to avoid having too many false positives on our indicator. Create two separate features, \"magicWordsOnce\" and \"magicWordsTwice\". The first feature indicates if any of the magic words appear in our text. The second feature indicates if at least two words from the text are in our magic words set. They do not have to be unique either, so a text that's just \"free free\" would also activate this indicator. Add both of these {0, 1} indicators to the `sms` dataframe. \n",
    "\n",
    "*Hint: Don't overcomplicate the regex for this problem.*<br>\n",
    "*Hint: str.lower() converts the string into lowercase.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN STUDENT SOLUTION\n",
    "\n",
    "# END STUDENT SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at these counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms.loc[:,[\"text\", \"label\", \"magicWordsOnce\"]].groupby(by=[\"label\", \"magicWordsOnce\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms.loc[:,[\"text\", \"label\", \"magicWordsTwice\"]].groupby(by=[\"label\", \"magicWordsTwice\"]).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that these are much better indicators than using just links, since these indicators for our magic words are much more likely to go off. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating our Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've processed our text, let's try to see how well our classifier performs using just these three features. We split our data into a train and test set and train a basic logistic regression classifier on the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = sms.drop([\"label\", \"text\"], axis = 1), sms[\"label\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression().fit(X_train, y_train)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_train, model.predict(X_train))\n",
    "train_auc = metrics.auc(fpr, tpr)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, model.predict(X_test))\n",
    "test_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "print(\"Train AUROC: \", train_auc)\n",
    "print(\"Test AUROC: \", test_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that you've added your features correctly, you should have an AUROC of above 0.9. This shows that our classifier is fairly capable, despite using a simple model! Hopefully these exercises provide a good foundation on how to process text with pandas and regex. \n",
    "\n",
    "As for some additional practice, one aspect of regex that this assignment did not take advantage of is capture groups. While the features didn't really care about what the exact matches were (ie. our features didn't need to know what the link matched was or what phone number was included in the spam mail); there are certain applications where the match's exact value is vital."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
