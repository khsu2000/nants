{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Null and Missing Values Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will run you through the problem of missing and null values when doing data cleaning. There are 2 approaches to handling missing/null values. The first is deletion where you delete the entire column associated with the null value and the second involves imputation to fill the missing values. There are many kinds of imputation such as summary statistic, linear regression and K-Nearest Neighbours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries and Dataset\n",
    "First, we import the necessary libraries and dataset. The dataset we are using for this assignment is Kaggle's Melbounre Housing dataset. The notebook is going to develop a model for finding the price of the house using the numerical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "raw_data = pd.read_csv('Melbourne_housing_FULL.csv')\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Describe what you notice from the header of the data?__ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is hard to deal with non numerical variables when running a training model as they only take numerical values. Some of the values can be changed into dummy values but others such as address are hard to be interpreted. __Drop the non numerical variables then remove the null values associated with Price.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin Soultion\n",
    "#End Solution\n",
    "full_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a function that helps count the missing values by columns which will allow us to see how the missing values are distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_vals(data):\n",
    "    missing_val_count_by_column = (data.isnull().sum())\n",
    "    return (missing_val_count_by_column[missing_val_count_by_column > 0])\n",
    "\n",
    "missing_vals(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping Columns and Rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The count in the cell above shows that there are many columns with missing values present in the data. This part of the notebook approaches the problem by deletion of rows and columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Drop the variables of YearBuilt and Building Area for the data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin Solution\n",
    "\n",
    "# End Solution\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Why do you think we should drop those variables instead of imputing into them?__ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_vals(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the functions for comparing the dataset and the train-test split of the data. This helper function is credited to https://www.kaggle.com/dansbecker/handling-missing-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_dataset(X_train, X_test, y_train, y_test):\n",
    "    model = RandomForestRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    return mean_absolute_error(y_test, preds)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop(columns = ['Price']), data['Price'], train_size=0.7, \n",
    "                                                    test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we shall explore deletion and imputation with our existing dataset. Let's try to drop all the rows with a null value and then try to drop all the columns with null values. First, we shall explore dropping the rows. We are not going to test the model of dropped rows due to different number of data points leading to different mean absolute errors. However, we will test the model with dropped columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_train_rows = data.copy(deep=True)\n",
    "# Begin Solution\n",
    "# End Solution\n",
    "print(missing_vals(dropped_rows))\n",
    "assert missing_vals(dropped_rows).sum() == 0\n",
    "dropped_rows.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There should be no missing values in the new dataset but now let us take a look at the number of points remaining in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_rows.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that we have removed around 1/3rd of the original dataset by dropping the rows with missing values. Consider the case where we did not drop the variables before dropping the rows with missing values. How many points would you expect?. Now lets us try this with columns.<br>\n",
    "_Hint : Find the missing columns across the entire dataset first and drop those columns for both train and test._ <br>\n",
    "__What do you expect to see if we drop the columns with missing datapoints?__ <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin Solution\n",
    "\n",
    "# End Solution\n",
    "assert missing_vals(dropped_train_cols).sum() == 0\n",
    "assert missing_vals(dropped_test_cols).sum() == 0\n",
    "dropped_train_cols.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now make a model using the remaining columns. Use the helper function score_dataset to run the model. <br>\n",
    "__Comment on the resultant mean absolute error__ <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin Solution\n",
    "score_dataset(dropped_train_cols, dropped_test_cols, y_train, y_test)\n",
    "# End Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation of values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputation of values involves filling in the values with other values such as the mean or a linear regression or KNN to find replacements. <br>\n",
    "__Describe those 3 different imputation methods__<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you need to implement a simple imputer which just uses the mean of each of the columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Begin Solution\n",
    "\n",
    "# End Solution\n",
    "score_dataset(imputed_X_train, imputed_X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are going to explore KNN in more detail. Implement a KNN model and explore different values for the number of nearest neighbours with uniform weights. __Comment on what you find.__ <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin Solution\n",
    "\n",
    "# End Solution\n",
    "score_dataset(KNN_X_train, KNN_X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further avenues of exploring imputation include marking imputation as a feature and adding it to the regression in addition to different imputation models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
