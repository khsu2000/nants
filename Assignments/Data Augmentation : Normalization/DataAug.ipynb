{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DataAug.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rew4a7WN2vZ0"
      },
      "source": [
        "# Image Data Augmentation and Normalization Notebook\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPLX3a9ZstSV"
      },
      "source": [
        "This notebook will run you through a real world example of image data augmentation and normalization. We will perform these techniques on CIFAR10, a lightweight 10-class image classification dataset commonly used for benchmarking performance. We will then run our augmented dataset through VGG11, a shorter variant of the VGG series of ConvNets that were state of the art circa 2014 and still serve as a reasonable educational example of a relatively straightforward ConvNet.\n",
        "\n",
        "As a side note, if your computer doesn't have GPU support, I would highly recommend running this on Google Colab, as you can change the runtime to use GPU there. Otherwise, the last section of the notebook will take multiple hours to run, so you can just skip it. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trZHfof32zK4"
      },
      "source": [
        "Let's first import some libraries. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_fPyONZ24s-"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.autograd import Variable\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5gHMfNnb5vZ"
      },
      "source": [
        "In addition, let's define some helpful tools and models. We will do all the heavy work of writing the neural net model definition, loading the dataset, and the training pipeline, as well as some visualization tools. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46M42k9_5y6x"
      },
      "source": [
        "cfg = {\n",
        "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "}\n",
        "\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, vgg_name):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = self._make_layers(cfg[vgg_name])\n",
        "        self.classifier = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "    def _make_layers(self, cfg):\n",
        "        layers = []\n",
        "        in_channels = 3\n",
        "        for x in cfg:\n",
        "            if x == 'M':\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            else:\n",
        "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
        "                           nn.BatchNorm2d(x),\n",
        "                           nn.ReLU(inplace=True)]\n",
        "                in_channels = x\n",
        "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "def test():\n",
        "    net = VGG('VGG11')\n",
        "    x = torch.randn(2,3,32,32)\n",
        "    y = net(x)\n",
        "    print(y.size())\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kI-TkaVMMgO0",
        "outputId": "b7db52a8-821a-4379-9a49-fff7f5c47f2d"
      },
      "source": [
        "test()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsD5ridC3I0A"
      },
      "source": [
        "def loadData(transform, train, batch_size):\n",
        "\n",
        "  dir = './data/train' if train else './data/val'\n",
        "  dataset = torchvision.datasets.CIFAR10(dir, \n",
        "                                             transform = transform,\n",
        "                                             train = train, \n",
        "                                             download=True)\n",
        "\n",
        "  loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=train)\n",
        "  return loader"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-dzVQpIOwZw"
      },
      "source": [
        "def train(net, criterion, epoch, optimizer, train_loader):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for batch_idx, (inputs, targets) in enumerate(tqdm(train_loader)):\n",
        "        inputs, targets = inputs.cuda(), targets.cuda() #comment out if no cuda\n",
        "        optimizer.zero_grad()\n",
        "        inputs, targets = Variable(inputs), Variable(targets)\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets.data).cpu().sum()\n",
        "\n",
        "    print('Training Loss: %.3f | Acc: %.3f%% (%d/%d)'  \n",
        "            % (train_loss/(batch_idx+1), \n",
        "            100.*float(correct)/float(total), correct, total))\n",
        "    \n",
        "    return train_loss / (batch_idx+1)\n",
        "\n",
        "def val(net, criterion, epoch, optimizer, val_loader, best_acc):\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(tqdm(val_loader)):\n",
        "        inputs, targets = inputs.cuda(), targets.cuda() #comment out if no cuda\n",
        "        with torch.no_grad():\n",
        "            inputs, targets = Variable(inputs), Variable(targets)\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        test_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets.data).cpu().sum()\n",
        "\n",
        "    print('Validation Loss: %.3f | Acc: %.3f%% (%d/%d)'  \n",
        "            % (test_loss/(batch_idx+1), \n",
        "            100.*float(correct)/float(total), correct, total))\n",
        "\n",
        "    acc = 100.*float(correct)/float(total)\n",
        "    if acc > best_acc:\n",
        "        best_acc = acc\n",
        "    return test_loss / (batch_idx+1)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ih7ZmTbx4C3J"
      },
      "source": [
        "def runTraining(train_transform, val_transform):\n",
        "    #load data\n",
        "    print('loading data...')\n",
        "\n",
        "    train_loader = loadData(train_transform, True, 128)\n",
        "    val_loader = loadData(val_transform, False, 128)\n",
        "    \n",
        "    #set up net\n",
        "    print('setting up net...')\n",
        "    net = VGG('VGG11')\n",
        "    \n",
        "    #set up cuda, comment out if no cuda\n",
        "    print('setting up cuda...')\n",
        "    net.cuda()\n",
        "    net = nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "    #set up training\n",
        "    print('setting up training...')\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(\n",
        "            net.parameters(), \n",
        "            lr=1e-2, \n",
        "            momentum=0.9, \n",
        "            weight_decay=1e-4) #can be changed in optional part\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, \n",
        "                                               milestones=[4], gamma=0.1) #can be changed in optional part\n",
        "\n",
        "    start_epoch = 0\n",
        "    max_epoch = 10 #can be changed in optional part\n",
        "    best_acc = 0\n",
        "\n",
        "    #run one round of inference before beginning testing\n",
        "    print('running pre-training inference...')\n",
        "    best_acc = val(net, criterion, start_epoch, optimizer, val_loader, best_acc)\n",
        "\n",
        "    #training\n",
        "    print('starting training...')\n",
        "    start_epoch += 1\n",
        "    max_epoch += 1\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    for epoch in range(start_epoch, max_epoch):\n",
        "        train_loss = train(net, criterion, epoch, optimizer, train_loader)\n",
        "        scheduler.step()\n",
        "        val_loss = val(net, criterion, epoch, optimizer, val_loader, best_acc)\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "    \n",
        "    return train_losses, val_losses\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5G6GErOMgXG1"
      },
      "source": [
        "def matplotlib_imshow(img, one_channel=False):\n",
        "    if one_channel:\n",
        "        img = img.mean(dim=0)\n",
        "    #img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    if one_channel:\n",
        "        plt.imshow(npimg, cmap=\"Greys\")\n",
        "    else:\n",
        "        plt.imshow(np.transpose(npimg, (1, 2, 0)))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_95g9iiXgskh"
      },
      "source": [
        "def showKImgs(transform, k):\n",
        "  # get some random training images\n",
        "  loader = loadData(transform, True, k)\n",
        "\n",
        "  dataiter = iter(loader)\n",
        "  images, labels = dataiter.next()\n",
        "\n",
        "  # create grid of images\n",
        "  img_grid = torchvision.utils.make_grid(images)\n",
        "\n",
        "  # show images\n",
        "  matplotlib_imshow(img_grid, one_channel=False)\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isfSvfJdic5n"
      },
      "source": [
        "Let's first define a dummy transform. This transform performs no augmentation except for the necessary task of converting the PIL images to tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aY_WB3sYiaKH"
      },
      "source": [
        "val_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "train_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "    ])\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJyyu54Zira_"
      },
      "source": [
        "Use the provided functions to visualize the first couple of images in the training dataset. We will be using the CIFAR10 dataset (this is already written for you, don't worry about it). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvnkBPtViqLu"
      },
      "source": [
        "#begin student code\n",
        "#end student code "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRo5FLFKjMwm"
      },
      "source": [
        "Here is the pytorch transforms documentation: https://pytorch.org/docs/stable/torchvision/transforms.html. Image transforms such as crops, flips, shifts, etc. should only ever be applied to the train set, as applying them to the val set as well defeats the purpose of augmenting the dataset in the first place. Try to use one of the functions in there to perform a random horizontal flip of the images, then visualize it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFGdtJU7iqKr"
      },
      "source": [
        "#begin student code\n",
        "\n",
        "#end student code"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2iVqGpxnanJ"
      },
      "source": [
        "In addition to image transforms, we may want to normalize or standardize the images. These need to be applied to both the val and train sets. To normalize, we first need to figure out the mean and stddev for each rgb channel across the entire dataset. Try calculating the mean and stddev for CIFAR10 first and print them out. \n",
        "\n",
        "Hint: Moving the raw data onto tensors scales the data from 0-255 to 0-1, so make sure to account for that. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93DWe2ABoF-b"
      },
      "source": [
        "#begin student code\n",
        "\n",
        "#end student code"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfIaCn-6ymQG"
      },
      "source": [
        "Now use your calculated mean and stddev to normalize the dataset using the function from transforms, and visualize the first couple of images. As a sanity check, you can look up the mean and stddev for CIFAR10 according to this thread: https://github.com/facebookarchive/fb.resnet.torch/issues/180"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFv_-ySXyyF8"
      },
      "source": [
        "#begin student code\n",
        "\n",
        "#end student code"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Zgsfqg9TDSm"
      },
      "source": [
        "As a baseline, we will see how well VGG performs on CIFAR10 without any augmentation. Write a dummy transform with no augmentation and use the given functions to run the training (this should take roughly 30 seconds per epoch for a total of roughly 5 minutes)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7nxJWwJOvZC"
      },
      "source": [
        "#begin student code\n",
        "\n",
        "#end student code"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OJQ9OOHB544"
      },
      "source": [
        "Plot the train and val losses per epoch using a line graph. You can use this as a reference: https://matplotlib.org/3.3.3/api/_as_gen/matplotlib.pyplot.plot.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZ36pCaHBGWO"
      },
      "source": [
        "#begin student code\n",
        "\n",
        "\n",
        "#end student code"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0YwaLXpTdjH"
      },
      "source": [
        "You should have gotten roughly 0.7 val loss. However, the trianing loss is almost zero, which means we have saturated performance on the training set. This is a classic example of overfitting, as our train loss is vastly lower than our val loss. After around epoch 5, we actually get much worse as we keep training because our model fits more and more towards the training data, which is not robust enough.\n",
        "\n",
        "Now, you will compose your own train_transform function and use it to train with your augmented dataset. Fill in the following cell and run the training (this will take a little longer, as applying the transforms for each image takes up some time as well).\n",
        "\n",
        "You can look at some preprocessing practices in section 4.1 of this classic paper: https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\n",
        "\n",
        "as well as section 3.4 of this other classic paper:\n",
        " https://arxiv.org/pdf/1512.03385.pdf\n",
        "\n",
        "Hint: check to make sure the final size of your images after the transform is done matches the expected input size from CIFAR10.\n",
        "\n",
        "The pytorch transforms documentation for your convenience: https://pytorch.org/docs/stable/torchvision/transforms.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUKU4ylQ7goi"
      },
      "source": [
        "#begin student code\n",
        "\n",
        "#end student code"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWR_-VCiDY9Z"
      },
      "source": [
        "Again, plot the train and val losses per epoch using a line graph. You can use this as a reference: https://matplotlib.org/3.3.3/api/_as_gen/matplotlib.pyplot.plot.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ao3IB9ICDcoW"
      },
      "source": [
        "#begin student code\n",
        "\n",
        "\n",
        "#end student code"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ioee3wKwcc3z"
      },
      "source": [
        "You should have gotten a better val loss, around 0.5 or so. In addition, the final train loss is still also around 0.5 or so. This means our model has not saturated train performance yet, so if we were to continue training it, it has room for improvement, which should also improve the val accuracy (we won't make you sit through this, as fully training out neural networks for real life performance takes many epochs and compute hours, and the payoffs are diminishing in return the longer you train). In short, our model is a lot less weak to overfitting on the training data.\n",
        "\n",
        "As an optional extra exercise, the canonical performance for VGG11 on CIFAR10 is around 91%. See if you can play around with different data transform options to come somewhere near that performance. You will probably need to tune some hyperparameters as well, such as the LRScheduler, LR, weight decay, epochs, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OK2VX7pUUT1L"
      },
      "source": [
        ""
      ],
      "execution_count": 18,
      "outputs": []
    }
  ]
}