{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE Tutorial #\n",
    "Sometimes, we suffer from having unbalanced datasets which may have some unexpected effects on our ability to classify. The following code just generates the dataset and the test data. Note that this is most devastating when the proportion of our test distribution is different than our training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu2 = [0, 3]\n",
    "mu1 = [3, 2]\n",
    "sigma1 = [[1,0], [0,1]]  # must be positive semi-definite\n",
    "sigma2 = [[.2,0],[0,.2]]\n",
    "n = 5000\n",
    "m = 50\n",
    "x1 = np.random.multivariate_normal(mu1, sigma1, size=n).T\n",
    "x2 = np.random.multivariate_normal(mu2, sigma2, size=m).T\n",
    "X_train = np.hstack([x1, x2])\n",
    "y_train = np.hstack([np.ones(n), -1*np.ones(m)])\n",
    "\n",
    "xt1 = np.random.multivariate_normal(mu1, sigma1, size=1000).T\n",
    "xt2 = np.random.multivariate_normal(mu2, sigma2, size=1000).T\n",
    "X_test = np.hstack([xt1, xt2])\n",
    "y_test = np.hstack([np.ones(1000), -1*np.ones(1000)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in our dataset, we have a 1:100 ratio for the minority class to the majority class in the test set, but a 1:1 ratio in our test distribution. Notice how the two classes also somewhat overlap, but the overlap is more due to the overwhelming amount of data points that the majority have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x1[0], x1[1])\n",
    "ax.scatter(x2[0], x2[1], c=\"red\")\n",
    "ax.set_aspect(\"equal\")\n",
    "#ax.set_xlim(-8, 8)\n",
    "#ax.set_ylim(-8, 8)\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.set_title(\"Raw Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will train a logistic regression model and see how well we do with this raw data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression().fit(X_train.T, y_train)\n",
    "train_acc = metrics.accuracy_score(y_train, model.predict(X_train.T))\n",
    "\n",
    "print(\"Train accuracy:\", train_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we have a 99% accuracy on our classifier. However, this is where the paradox of accuracy comes into play. Let us look at the accuracy for the test set and the AUROC scores for each of the test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(y_train, model.predict(X_train.T))\n",
    "train_auc = metrics.auc(fpr, tpr)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, model.predict(X_test.T))\n",
    "test_auc = metrics.auc(fpr, tpr)\n",
    "test_acc = metrics.accuracy_score(y_test, model.predict(X_test.T))\n",
    "\n",
    "print(\"Train AUROC: \", train_auc)\n",
    "print(\"Test accuracy:\", test_acc)\n",
    "print(\"Test AUROC: \", test_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our test accuracy isn't great consider how good the training accuracy is. Let us plot the classifications and see how the classification boundary seems to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(X, y):\n",
    "    X = X.T\n",
    "    class1 = X[np.where(y == 1)].T\n",
    "    class2 = X[np.where(y == -1)].T\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(class1[0], class1[1])\n",
    "    ax.scatter(class2[0], class2[1], c=\"red\")\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.set_title(\"Results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(X_train, model.predict(X_train.T))\n",
    "plot(X_test, model.predict(X_test.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see where our problems lie. Because there are so little samples of the minority class, the algorithm is favoring the majority class in training. Thus, when we go to the test set, we can very clearly see that while the two blobs are very clearly seperable, the algorithm takes a chunk out of the minority class's data points. \n",
    "\n",
    "This is a problem that can be solved if we had access to more data of the minority class at training time. Thus, we shall implement SMOTE like in the notes to try and mitigate the following problem. Implement the following SMOTE method. The parameter $X$ is the dataset that we are basing the algorithm on, the parameter $k$ is the number of neighbors for the nearest neighbor classifier, the parameter $a$ is used to calculate the convex hull of the two samples and $num\\_samples$ should be the number of synthetic samples to generate. Note that sklearn.neighbors.NearestNeighbors has been imported and should allow for efficient nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def SMOTE(X, k, a, num_samples):\n",
    "    ##Code starts here ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have generated this algorithm, let us try and visualize the samples that SMOTE generated. If you did it correctly, you should see a black mass of points where the minority class is centered around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = SMOTE(x2, 5, 0.5, 3000)\n",
    "xs = xs.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x1[0], x1[1], alpha=0.2)\n",
    "ax.scatter(x2[0], x2[1], c=\"red\")\n",
    "ax.scatter(xs[0], xs[1], c=\"black\", alpha=0.2)\n",
    "ax.set_aspect(\"equal\")\n",
    "#ax.set_xlim(-8, 8)\n",
    "#ax.set_ylim(-8, 8)\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.set_title(\"Raw Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try and see how this newly augmented training set does at training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_aug = np.hstack([X_train, xs])\n",
    "y_train_aug = np.hstack([y_train, -1 * np.ones(3000)])\n",
    "\n",
    "model = LogisticRegression().fit(X_train_aug.T, y_train_aug)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_train, model.predict(X_train.T))\n",
    "train_acc = metrics.accuracy_score(y_train, model.predict(X_train.T))\n",
    "train_auc = metrics.auc(fpr, tpr)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, model.predict(X_test.T))\n",
    "test_auc = metrics.auc(fpr, tpr)\n",
    "test_acc = metrics.accuracy_score(y_test, model.predict(X_test.T))\n",
    "\n",
    "print(\"Train accuracy:\", train_acc)\n",
    "print(\"Test accuracy:\", test_acc)\n",
    "print(\"Train AUROC: \", train_auc)\n",
    "print(\"Test AUROC: \", test_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(X_train, model.predict(X_train.T))\n",
    "plot(X_test, model.predict(X_test.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that after we augment the training set, we can see that our model does much better on the test set in exchange for a little bit of loss on the original training set. Thus, we can see how adding synthetic samples to augment a training set can help us do better at test time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 ##\n",
    "Can you think of any case where SMOTE would not be a good algorithm to use on the data points? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student Answer ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 ##\n",
    "\n",
    "Can you think of any case where we wouldn't want to rebalance the classes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student Answer ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
